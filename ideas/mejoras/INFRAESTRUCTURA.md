INFRAESTRUCTURA, CI/CD Y BACKUPS ‚Äì Versi√≥n Corregida

Este documento actualiza la gu√≠a de infraestructura, despliegues,
integraci√≥n continua (CI/CD) y pol√≠ticas de backup de ofitec.ai,
aline√°ndola con las leyes oficiales y las conclusiones del diagn√≥stico.

üö¢ Cumplimiento de Ley de Puertos

La infraestructura debe garantizar que s√≥lo existen dos puertos de
exposici√≥n en el sistema principal:

Puerto 3001: Frontend Next.js. Todas las p√°ginas y el portal
cliente deben servirse desde aqu√≠„Äê8‚Ä†L43-L50„Äë. Durante la
construcci√≥n/ejecuci√≥n, se debe configurar el servidor de
desarrollo (npm run dev) para que corra en 3001 tanto en
desarrollo como en producci√≥n.

Puerto 5555: Backend Flask. Todas las APIs REST, incluyendo
QHSE, Finanzas, SII, Anal√≠tica y Portal, deben exponerse en este
puerto„Äê8‚Ä†L43-L50„Äë. Se proh√≠ben servidores paralelos (3000, 3002
o 8000).

En la pipeline de despliegue, agregar un paso de verificaci√≥n
autom√°tica que ejecute:

netstat -an | grep ':3001' || exit 1
netstat -an | grep ':5555' || exit 1


Si se detecta un puerto no autorizado, se aborta el despliegue.

üõ†Ô∏è CI/CD

El pipeline de integraci√≥n y entrega continua (CI/CD) debe incluir:

Pruebas unitarias y de integraci√≥n: ejecutar pytest para el
backend (Flask) y tests con Vitest/Jest para el frontend. Generar
reportes de cobertura. Cualquier falla detiene la pipeline.

Linting y Format: aplicar ruff/black en Python y ESLint +
Prettier en JavaScript/TypeScript antes de build.

Build Docker: construir las im√°genes del backend y frontend.
Asegurar que la base de datos de producci√≥n se conecta a la
instancia PostgreSQL (no SQLite). Usar multi‚Äëstage builds para
reducir el tama√±o.

Deploy Blue/Green: implementar despliegue de dos entornos
(blue/green) con un balanceador. Cambiar el alias DNS una vez
que el nuevo entorno pase todas las pruebas de salud. Basarse en
las entregas de autoscaling y blue/green (entrega¬†9)„Äê395‚Ä†source„Äë.

Rotaci√≥n de Secretos y Variables: integrar la pipeline con
Vault para obtener variables sensibles (contrase√±as DB,
credenciales SSO, llaves JWT). Nunca se guardan secretos en
repositorios.

Ejecuci√≥n de Scripts de BD: antes del deploy, correr los
scripts de verificaci√≥n de integridad (verificar_estado_db.py) y
recreaci√≥n de vistas can√≥nicas (create_finance_views.py)
en el entorno temporal. Abortarla si falla alguna verificaci√≥n.

Backup Pre‚ÄëDeploy: antes de migrar la base de datos, hacer un
backup transaccional (por ejemplo, pg_dump --format=custom). En
caso de fallar, se restaura inmediatamente el backup y se
cancela el despliegue.

La definici√≥n del workflow de GitHub Actions (o la herramienta
equivalente) debe incluir estos pasos. Utilice jobs secuenciados
con needs: para asegurar el orden correcto.

üîÑ Backup y Recuperaci√≥n (3‚Äë2‚Äë1)

Para cumplir el Art√≠culo¬†VII de la Ley de Base de Datos„Äê19‚Ä†L233-L241„Äë,
se implementa la estrategia 3‚Äë2‚Äë1:

3 Copias: mantener tres copias de cada backup:

Original (Base de Datos en Prod) ‚Äì replicada en modo WAL.

Backup Primario Local ‚Äì guardado en un volumen dedicado.

Backup Secundario Remoto ‚Äì almacenado cifrado en un bucket
(S3, GCS u OSS) en otra regi√≥n.

2 Medios Diferentes: una copia en disco local y otra en
servicio cloud. La copia local se usa para restauraciones r√°pidas.

1 Offsite: la copia remota (offsite) se mantiene en una regi√≥n
distinta para recuperaci√≥n ante desastres (DR).

Frecuencia de Backups

Transaccional: antes de cada importaci√≥n masiva (Chipax
Migration, ETL) o actualizaci√≥n de modelo.

Horario: backups incrementales cada hora durante el horario
laboral. Se pueden aprovechar los WAL archives y pgBackRest.

Diario: backup completo a las 02:00 AM (cron job)„Äê19‚Ä†L233-L241„Äë.

Semanal: backup completo comprimido y verificado; se mantiene
durante 4¬†semanas.

Mensual: backup hist√≥rico comprimido; retenci√≥n de 12¬†meses.

Todos los backups deben pasar por un proceso de verificaci√≥n de
integridad (pg_restore --list y pruebas de checksum). Se debe
probar peri√≥dicamente la restauraci√≥n en un entorno de staging.

Procedimiento de Recuperaci√≥n

Si se detecta corrupci√≥n de datos o se necesita volver a un estado
previo:

STOP inmediato: detener todas las operaciones del backend y
deshabilitar escritura en la BD„Äê19‚Ä†L258-L266„Äë.

Identificar punto de restauraci√≥n: seleccionar el backup m√°s
reciente v√°lido y los WAL necesarios.

Restaurar: usar pg_restore para la copia completa y aplicar
los WAL para alcanzar el punto deseado.

Verificar: correr los scripts de integridad y un subset de
pruebas de negocio (carga de dashboards, creaci√≥n de notas de
venta, etc.).

Reapertura: una vez verificada la base, reactivar el servicio
y notificar a los usuarios.

Reporte: documentar la causa, la duraci√≥n de la interrupci√≥n y
las acciones correctivas.

üìà Monitorizaci√≥n y Observabilidad

Se despliega un stack de observabilidad (Prometheus, Grafana y Loki)
con las siguientes caracter√≠sticas:

Exporters: se exponen m√©tricas del backend (latencias,
errores), del frontend (tiempo de renderizado) y de la base de
datos (con postgres_exporter). Para Odoo (si se mantiene alguna
pieza), se usa odoo_exporter„Äê11‚Ä†L251-L260„Äë.

Paneles Predise√±ados: se incluyen dashboards para monitorizar:

KPIs de sistemas (CPU, memoria, disco, latencia HTTP).

Estado de jobs de ETL y backups.

Salud de microservicios (QHSE, Anal√≠tica, etc.).

M√©tricas de negocio (√≥rdenes procesadas, notas de venta emitidas).

Alertas: configurar alertas en Prometheus para:

P√©rdida de conexi√≥n a BD (Alerta cr√≠tica)„Äê19‚Ä†L233-L241„Äë.

Fallos en backups autom√°ticos.

Incremento en duplicados detectados por la capa de validaci√≥n„Äê17‚Ä†L154-L163„Äë.

Saturaci√≥n del puerto 3001 o 5555.

Se recomienda enviar alertas a Slack y correo electr√≥nico, utilizando
las integraciones ya definidas en las entregas de autoscaling y
monitorizaci√≥n„Äê389‚Ä†source„Äë.

‚úÖ Resumen de Cumplimiento

Unificaci√≥n de Puertos: se aseguran las reglas 1‚Äì3 de la
Ley de Puertos„Äê8‚Ä†L43-L50„Äë.

Integridad y Backups: se implementa la estrategia 3‚Äë2‚Äë1, los
backups transaccionales y procedimientos de recuperaci√≥n conforme
a la Ley de Base de Datos„Äê19‚Ä†L233-L241„Äë.

CI/CD Rigurosa: el pipeline ejecuta pruebas, verifica
integridad de BD y evita que se desplieguen builds con
configuraciones incorrectas. Se incorporan scripts de
verificaci√≥n de vistas y bases de datos can√≥nicas.

Monitorizaci√≥n y Alertas: se instrumentan los servicios y se
configuran alertas para incidentes cr√≠ticos y degradaciones.

Con estas directrices, la infraestructura y el ciclo de vida de
despliegue de ofitec.ai se fortalecen, eliminando riesgos de puertos
err√≥neos, p√©rdida de datos y fallos silenciosos, y cumpliendo las
normas oficiales.